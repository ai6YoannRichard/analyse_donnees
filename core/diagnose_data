"""
Script de diagnostic et correction pour fichiers Excel (.xlsx)
Résout le problème de colonnes manquantes dans le pipeline d'analyse
"""

import pandas as pd
import logging
from pathlib import Path
import sys

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def diagnose_excel_data(file_path: str, sheet_name=None) -> dict:
    """
    Diagnostique les colonnes disponibles dans votre fichier Excel
    
    Args:
        file_path: Chemin vers le fichier Excel
        sheet_name: Nom de la feuille à analyser (None = première feuille)
    """
    try:
        # Charger le fichier Excel
        logger.info(f"Chargement du fichier Excel: {file_path}")
        
        # D'abord, lister les feuilles disponibles
        excel_file = pd.ExcelFile(file_path)
        logger.info(f"Feuilles disponibles: {excel_file.sheet_names}")
        
        # Charger la feuille spécifiée ou la première
        if sheet_name:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            logger.info(f"Feuille chargée: {sheet_name}")
        else:
            df = pd.read_excel(file_path)
            logger.info(f"Feuille chargée: {excel_file.sheet_names[0]}")
        
        logger.info(f"Données chargées: {df.shape[0]} lignes, {df.shape[1]} colonnes")
        
        # Analyser les colonnes
        diagnostic = {
            'file_path': file_path,
            'sheets': excel_file.sheet_names,
            'shape': df.shape,
            'columns': list(df.columns),
            'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
            'missing_values': df.isnull().sum().to_dict(),
            'numeric_columns': list(df.select_dtypes(include=['number']).columns),
            'text_columns': list(df.select_dtypes(include=['object']).columns),
            'date_columns': [],
            'suggested_mappings': {},
            'data_sample': df.head(3).to_dict()
        }
        
        # Détecter les colonnes de date
        for col in df.columns:
            if 'date' in str(col).lower() or 'dt' in str(col).lower():
                diagnostic['date_columns'].append(col)
            # Vérifier aussi le type datetime
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                if col not in diagnostic['date_columns']:
                    diagnostic['date_columns'].append(col)
        
        # Vérifier les colonnes critiques attendues
        expected_columns = {
            'montant_charge_brut': ['montant', 'charge', 'cout', 'cost', 'amount', 'sinistre', 'brut', 'total'],
            'date_sinistre': ['date', 'sinistre', 'claim', 'occurrence', 'evenement'],
            'surface_police': ['surface', 'area', 'superficie', 'size', 'm2', 'metre'],
            'risque_rga_gcl': ['risque', 'rga', 'gcl', 'retrait', 'gonflement', 'risk', 'argile'],
            'classe_age': ['age', 'classe', 'class', 'anciennete', 'ancien', 'construction'],
            'classe_surface': ['classe', 'surface', 'categorie', 'category', 'taille'],
            'cpostal': ['code', 'postal', 'cp', 'zip', 'postcode'],
            'ville': ['ville', 'city', 'commune', 'localite']
        }
        
        logger.info("\n" + "="*60)
        logger.info("DIAGNOSTIC DES COLONNES CRITIQUES")
        logger.info("="*60)
        
        for expected_col, keywords in expected_columns.items():
            found = False
            
            # Vérifier si la colonne existe exactement
            if expected_col in df.columns:
                logger.info(f"✓ '{expected_col}' trouvée directement")
                found = True
            else:
                # Chercher des colonnes similaires
                similar_cols = []
                for col in df.columns:
                    col_str = str(col).lower().replace('_', ' ').replace('-', ' ')
                    for keyword in keywords:
                        if keyword.lower() in col_str:
                            similar_cols.append(col)
                            break
                
                if similar_cols:
                    logger.warning(f"⚠ '{expected_col}' manquante, mais colonnes similaires trouvées:")
                    for sim_col in similar_cols[:3]:  # Limiter à 3 suggestions
                        logger.warning(f"   → {sim_col}")
                    diagnostic['suggested_mappings'][expected_col] = similar_cols[0]
                else:
                    logger.error(f"✗ '{expected_col}' manquante et aucune colonne similaire")
        
        # Afficher les premières colonnes disponibles
        logger.info("\n" + "="*60)
        logger.info("COLONNES DISPONIBLES DANS VOS DONNÉES")
        logger.info("="*60)
        
        for i, col in enumerate(df.columns[:20], 1):  # Afficher max 20 colonnes
            dtype = df[col].dtype
            null_count = df[col].isnull().sum()
            null_pct = (null_count / len(df)) * 100
            logger.info(f"{i:2d}. {col} ({dtype}) - {null_pct:.1f}% manquants")
        
        if len(df.columns) > 20:
            logger.info(f"... et {len(df.columns) - 20} colonnes supplémentaires")
        
        # Statistiques sur les variables cibles potentielles
        target_candidates = [col for col in df.columns 
                            if any(kw in str(col).lower() for kw in ['montant', 'charge', 'cout', 'cost', 'amount', 'valeur'])]
        
        if target_candidates:
            logger.info("\n" + "="*60)
            logger.info("VARIABLES CIBLES POTENTIELLES (montants)")
            logger.info("="*60)
            
            for col in target_candidates:
                if pd.api.types.is_numeric_dtype(df[col]):
                    non_null = df[col].dropna()
                    if len(non_null) > 0:
                        logger.info(f"\n{col}:")
                        logger.info(f"  Valeurs non nulles: {len(non_null)}/{len(df)} ({len(non_null)/len(df)*100:.1f}%)")
                        logger.info(f"  Moyenne: {non_null.mean():,.2f}")
                        logger.info(f"  Médiane: {non_null.median():,.2f}")
                        logger.info(f"  Min: {non_null.min():,.2f}")
                        logger.info(f"  Max: {non_null.max():,.2f}")
        
        return diagnostic
        
    except Exception as e:
        logger.error(f"Erreur lors du diagnostic: {e}")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}


def fix_and_run_pipeline(file_path: str, sheet_name=None, output_dir: str = './outputs_fixed'):
    """
    Execute le pipeline avec les corrections automatiques pour Excel
    
    Args:
        file_path: Chemin vers le fichier Excel
        sheet_name: Nom de la feuille à analyser
        output_dir: Répertoire de sortie pour les résultats
    """
    try:
        # Importer le pipeline
        logger.info("Import du pipeline...")
        from analyse.core.pipeline import SinistreAnalysisPipeline
        
        # 1. Diagnostic
        logger.info("\n" + "="*60)
        logger.info("ÉTAPE 1: DIAGNOSTIC DU FICHIER EXCEL")
        logger.info("="*60)
        
        diagnostic = diagnose_excel_data(file_path, sheet_name)
        
        if 'error' in diagnostic:
            logger.error(f"Erreur lors du diagnostic: {diagnostic['error']}")
            return None
        
        # 2. Configuration du pipeline
        logger.info("\n" + "="*60)
        logger.info("ÉTAPE 2: CONFIGURATION DU PIPELINE")
        logger.info("="*60)
        
        config = {
            'output_dir': output_dir,
            'generate_advanced_reports': False,  # Désactiver temporairement les rapports avancés
            'filter_params': {
                'apply_quality': True,
                'apply_business': False
            }
        }
        
        pipeline = SinistreAnalysisPipeline(config)
        
        # 3. Charger les données Excel
        logger.info("\n" + "="*60)
        logger.info("ÉTAPE 3: CHARGEMENT DES DONNÉES EXCEL")
        logger.info("="*60)
        
        # Charger avec la bonne feuille si spécifiée
        if sheet_name:
            pipeline.load_data(file_path, sheet_name=sheet_name)
        else:
            pipeline.load_data(file_path)
        
        # 4. Appliquer les corrections de colonnes
        logger.info("\n" + "="*60)
        logger.info("ÉTAPE 4: CORRECTION DES COLONNES MANQUANTES")
        logger.info("="*60)
        
        if pipeline.raw_data is not None:
            df = pipeline.raw_data
            
            # Appliquer les mappings suggérés
            if diagnostic['suggested_mappings']:
                for expected_col, actual_col in diagnostic['suggested_mappings'].items():
                    if actual_col in df.columns and expected_col not in df.columns:
                        logger.info(f"Renommage: '{actual_col}' → '{expected_col}'")
                        df.rename(columns={actual_col: expected_col}, inplace=True)
            
            # Ajouter les colonnes manquantes essentielles
            required_columns = {
                'risque_rga_gcl': 'NON_DEFINI',
                'classe_age': 'NON_DEFINI', 
                'classe_surface': 'NON_DEFINI'
            }
            
            for col, default_value in required_columns.items():
                if col not in df.columns:
                    logger.info(f"Ajout colonne '{col}' avec valeur par défaut '{default_value}'")
                    df[col] = default_value
            
            pipeline.raw_data = df
            pipeline.processed_data = df.copy()
        
        # 5. Identifier la variable cible
        logger.info("\n" + "="*60)
        logger.info("ÉTAPE 5: IDENTIFICATION DE LA VARIABLE CIBLE")
        logger.info("="*60)
        
        target_var = None
        target_candidates = ['montant_charge_brut', 'montant_sinistre', 'montant', 'charge_brut', 'cout_total']
        
        for col in target_candidates:
            if col in pipeline.raw_data.columns:
                target_var = col
                logger.info(f"✓ Variable cible trouvée: '{target_var}'")
                break
        
        if not target_var:
            # Chercher dans les colonnes suggérées
            if 'montant_charge_brut' in diagnostic['suggested_mappings']:
                target_var = 'montant_charge_brut'
                logger.info(f"✓ Variable cible après mapping: '{target_var}'")
            else:
                # Utiliser la première colonne numérique
                numeric_cols = pipeline.raw_data.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    target_var = numeric_cols[0]
                    logger.warning(f"⚠ Variable cible par défaut: '{target_var}'")
                else:
                    raise ValueError("Aucune variable numérique trouvée pour servir de cible")
        
        # 6. Exécuter l'analyse complète
        logger.info("\n" + "="*60)
        logger.info("ÉTAPE 6: EXÉCUTION DE L'ANALYSE")
        logger.info("="*60)
        
        # Exécution étape par étape avec gestion d'erreurs
        try:
            pipeline.filter_data(apply_quality=True, apply_business=False)
            logger.info("✓ Filtrage des données effectué")
        except Exception as e:
            logger.warning(f"⚠ Problème lors du filtrage: {e}")
        
        try:
            pipeline.setup_variables(target_var)
            logger.info("✓ Variables configurées")
        except Exception as e:
            logger.warning(f"⚠ Problème lors de la configuration des variables: {e}")
        
        try:
            pipeline.preprocess_data(apply_encoding=True)
            logger.info("✓ Préprocessing effectué")
        except Exception as e:
            logger.warning(f"⚠ Problème lors du préprocessing: {e}")
        
        try:
            pipeline.analyze_statistics(correlation_threshold=0.05)
            logger.info("✓ Analyses statistiques effectuées")
        except Exception as e:
            logger.warning(f"⚠ Problème lors des analyses statistiques: {e}")
        
        try:
            pipeline.create_visualizations()
            logger.info("✓ Visualisations créées")
        except Exception as e:
            logger.warning(f"⚠ Problème lors de la création des visualisations: {e}")
        
        try:
            pipeline.generate_synthesis()
            logger.info("✓ Synthèse générée")
        except Exception as e:
            logger.warning(f"⚠ Problème lors de la génération de la synthèse: {e}")
        
        # 7. Export des résultats
        logger.info("\n" + "="*60)
        logger.info("ÉTAPE 7: EXPORT DES RÉSULTATS")
        logger.info("="*60)
        
        results = pipeline.export_results('all')
        
        logger.info("\n" + "="*60)
        logger.info("✅ ANALYSE TERMINÉE AVEC SUCCÈS")
        logger.info("="*60)
        logger.info(f"\nRésultats exportés dans: {output_dir}")
        
        for key, path in results.items():
            logger.info(f"  → {key}: {path}")
        
        return results
        
    except Exception as e:
        logger.error(f"\n❌ Erreur lors de l'exécution: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # Configuratio
    # n - MODIFIEZ CES VALEURS
    PROJECT_ROOT = Path(__file__).parent.parent
    DATA_DIR = PROJECT_ROOT / "data"
    EXCEL_FILE = DATA_DIR  / "run_04_dataset_imputed.xlsx"  # ← REMPLACEZ PAR LE CHEMIN DE VOTRE FICHIER
    SHEET_NAME = None  # ← None = première feuille, ou spécifiez le nom de la feuille
    OUTPUT_DIR = "./resultats_analyse"  # ← Dossier de sortie pour les résultats
    
    print("\n" + "="*80)
    print("ANALYSE DE SINISTRES - FICHIER EXCEL")
    print("="*80)
    
    # Option 1: Diagnostic seul
    print("\n[1] DIAGNOSTIC DU FICHIER")
    diagnostic_result = diagnose_excel_data(EXCEL_FILE, SHEET_NAME)
    
    if 'error' not in diagnostic_result:
        # Option 2: Exécuter l'analyse complète
        print("\n" + "="*80)
        response = input("\nVoulez-vous lancer l'analyse complète avec corrections automatiques? (o/n): ")
        
        if response.lower() in ['o', 'oui', 'y', 'yes']:
            print("\n[2] LANCEMENT DE L'ANALYSE COMPLÈTE")
            fix_and_run_pipeline(EXCEL_FILE, SHEET_NAME, OUTPUT_DIR)
        else:
            print("\nAnalyse annulée. Le diagnostic a été sauvegardé.")
    else:
        print(f"\n❌ Erreur: {diagnostic_result['error']}")
        print("Vérifiez que le fichier existe et est accessible.")